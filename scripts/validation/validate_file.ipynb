{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e7adc341-48f6-4d16-8d08-e6652bce0a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103143 packets\n",
      "Start time sample = 55131776, total time samples = 825089, (= total time 0.89109612 seconds)\n",
      "Start channel = 252, total channels = 8\n",
      "expected packets = 103136\n",
      "stopping packet decoding at packet 900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import dpkt\n",
    "import struct\n",
    "import typing\n",
    "import argparse\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "def get_udp_payload_bytes(pcap_filename) -> typing.List[bytes]:\n",
    "    \"\"\"\n",
    "    Read UDP payload bytes from a .pcap(ng) file\n",
    "    return list with one entry per packet, entry containing udp payload\n",
    "    \"\"\"\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(pcap_filename, \"rb\") as f:\n",
    "            pcap_read = dpkt.pcap.UniversalReader(f)\n",
    "            udp_payloads = list()\n",
    "            for ts, buf in pcap_read:\n",
    "                # skip packet if not long enough to contain IP+UDP+CODIF hdrs\n",
    "                if len(buf) < (34 + 8 + 64):\n",
    "                    print(f\"WARNING: Found packet that is too small {len(buf)}Bytes\")\n",
    "                    continue\n",
    "                eth = dpkt.ethernet.Ethernet(buf)\n",
    "                ip = eth.data\n",
    "                # skip non-UDP packets\n",
    "                if ip.p != dpkt.ip.IP_PROTO_UDP:\n",
    "                    print(f\"WARNING: Found packet that is not UDP {ip.p} type\")\n",
    "                    continue\n",
    "                # add the UDP payload data into the list of payloads\n",
    "                udp = ip.data\n",
    "                udp_payloads.append(udp.data)\n",
    "                timestamps.append(ts)\n",
    "    except FileNotFoundError as fnf_err:\n",
    "        print(fnf_err)\n",
    "        sys.exit(1)\n",
    "\n",
    "    return timestamps, udp_payloads\n",
    "\n",
    "\n",
    "N_POL = 2\n",
    "N_VALS_PER_CPLX = 2\n",
    "N_BYES_PER_VAL = 1\n",
    "N_BYTES_PER_SAMPLE = N_POL * N_VALS_PER_CPLX * N_BYES_PER_VAL\n",
    "\n",
    "\n",
    "def get_channel_power(beam, payload):\n",
    "    seq_no = struct.unpack(\"Q\", payload[0:8])[0]\n",
    "    scale1 = struct.unpack(\"f\", payload[32:36])[0]\n",
    "    first_chan = struct.unpack(\"I\", payload[48:52])[0]\n",
    "    num_chan = struct.unpack(\"H\", payload[52:54])[0]\n",
    "    valid_chan = struct.unpack(\"H\", payload[54:56])[0]\n",
    "    num_sample = struct.unpack(\"H\", payload[56:58])[0]\n",
    "    beam_id = struct.unpack(\"H\", payload[58:60])[0]\n",
    "    sample_per_weight = struct.unpack(\"B\", payload[67:68])[0]\n",
    "    if beam_id != beam:\n",
    "        return None\n",
    "    weights_offset = 96  # start of weights (multiple of 16bytes=128 bits)\n",
    "    n_weight_bytes = num_sample / sample_per_weight * 2 * num_chan\n",
    "    # weights padded to multiple of 128 bits = 16 bytes\n",
    "    data_offset = weights_offset + math.ceil(n_weight_bytes / 16) * 16\n",
    "\n",
    "    np_chanl_pwr = np.zeros((2, num_chan))\n",
    "    for ch in range(0, valid_chan):\n",
    "        ch_offset = data_offset + (\n",
    "            ch * num_sample * N_BYES_PER_VAL * N_VALS_PER_CPLX * N_POL\n",
    "        )\n",
    "        # sum sample power for both polarisations\n",
    "        for pol in range(0, 2):\n",
    "            pol_base = ch_offset + pol * num_sample * N_BYES_PER_VAL * N_VALS_PER_CPLX\n",
    "            for sample_idx in range(0, num_sample):\n",
    "                loc_x = pol_base + sample_idx * 2\n",
    "                x_i, x_q = struct.unpack(\"bb\", payload[loc_x : loc_x + 2])\n",
    "                sample_pwr = x_i * x_i + x_q * x_q\n",
    "                np_chanl_pwr[pol][ch] += sample_pwr\n",
    "    # average power per-complex-sample per channel\n",
    "    np_chanl_pwr = np_chanl_pwr / (scale1 * scale1) / valid_chan / num_sample\n",
    "    # print(f\"scale: {scale1}\")\n",
    "    # print(f\"weights_bytes: {n_weight_bytes} data_offset: {data_offset} + {valid_chan*pol*num_sample}samples\")\n",
    "    # print(f\"pol_base {pol_base}+{num_sample*2}\")\n",
    "\n",
    "    return (seq_no, first_chan, np_chanl_pwr)\n",
    "\n",
    "\n",
    "def get_packet_data(payload):\n",
    "    seq_no = struct.unpack(\"Q\", payload[0:8])[0]\n",
    "    sample_no = struct.unpack(\"Q\", payload[8:16])[0]\n",
    "    scale1 = struct.unpack(\"f\", payload[32:36])[0]\n",
    "    first_chan = struct.unpack(\"I\", payload[48:52])[0]\n",
    "    num_chan = struct.unpack(\"H\", payload[52:54])[0]\n",
    "    valid_chan = struct.unpack(\"H\", payload[54:56])[0]\n",
    "    num_sample = struct.unpack(\"H\", payload[56:58])[0]\n",
    "    beam_id = struct.unpack(\"H\", payload[58:60])[0]\n",
    "    sample_per_weight = struct.unpack(\"B\", payload[67:68])[0]\n",
    "\n",
    "    weights_offset = 96  # start of weights (multiple of 16bytes=128 bits)\n",
    "    n_weight_bytes = num_sample / sample_per_weight * 2 * num_chan\n",
    "    # weights padded to multiple of 128 bits = 16 bytes\n",
    "    data_offset = weights_offset + math.ceil(n_weight_bytes / 16) * 16\n",
    "    # samples for one packet = (channels) x (2 pol) x (time samples)\n",
    "    samples = np.zeros((valid_chan, 2, num_sample), dtype=np.complex64)\n",
    "\n",
    "    for ch in range(0, valid_chan):\n",
    "        ch_offset = data_offset + (\n",
    "            ch * num_sample * N_BYES_PER_VAL * N_VALS_PER_CPLX * N_POL\n",
    "        )\n",
    "        for pol in range(0, 2):\n",
    "            pol_base = ch_offset + pol * num_sample * N_BYES_PER_VAL * N_VALS_PER_CPLX\n",
    "            for sample_idx in range(0, num_sample):\n",
    "                loc_x = pol_base + sample_idx * 2\n",
    "                x_i, x_q = struct.unpack(\"bb\", payload[loc_x : loc_x + 2])\n",
    "                samples[ch, pol, sample_idx] = 1j * np.float32(x_q) + np.float32(x_i)\n",
    "    return (seq_no, sample_no, beam_id, first_chan, scale1, samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lambda_file = \"cap_13Dec2024_0.pcapng\"\n",
    "total_ADCs = 20\n",
    "pcount_max = 900\n",
    "\n",
    "# read pcap file\n",
    "tstamps, payloads = get_udp_payload_bytes(lambda_file)\n",
    "\n",
    "# run through the data to find the number of beams and channels\n",
    "first_pkt = True\n",
    "start_seq_no = 0\n",
    "start_chan = 0\n",
    "end_seq_no = 0\n",
    "end_chan = 0\n",
    "total_packets = 0\n",
    "\n",
    "for ts, pkt_payload in zip(tstamps, payloads):\n",
    "    seq_no = struct.unpack(\"<Q\", pkt_payload[0:8])[0]\n",
    "    FPGA_id = struct.unpack(\"<I\", pkt_payload[8:12])[0]\n",
    "    freq_chan = struct.unpack(\"<H\", pkt_payload[12:14])[0]\n",
    "    total_packets += 1\n",
    "    if first_pkt:\n",
    "        first_pkt = False\n",
    "        start_seq_no = seq_no\n",
    "        end_seq_no = seq_no\n",
    "        start_chan = freq_chan\n",
    "        end_chan = freq_chan\n",
    "    else:\n",
    "        if freq_chan < start_chan:\n",
    "            start_chan = freq_chan\n",
    "        if seq_no < start_seq_no:\n",
    "            start_seq_no = seq_no\n",
    "        if freq_chan > end_chan:\n",
    "            end_chan = freq_chan\n",
    "        if seq_no > end_seq_no:\n",
    "            end_seq_no = seq_no\n",
    "\n",
    "print(f\"Found {total_packets} packets\")\n",
    "print(\n",
    "    f\"Start time sample = {start_seq_no}, total time samples = {end_seq_no - start_seq_no + 1}, (= total time {1080e-9 * (end_seq_no - start_seq_no + 1)} seconds)\"\n",
    ")\n",
    "print(\n",
    "    f\"Start channel = {start_chan}, total channels = {(end_chan - start_chan) + 1}\"\n",
    ")\n",
    "total_channels = (end_chan - start_chan) + 1\n",
    "total_time_packets = (end_seq_no - start_seq_no + 1) // 64\n",
    "expected_packets = total_channels * total_time_packets\n",
    "print(f\"expected packets = {expected_packets}\")\n",
    "\n",
    "# Get all the data into a big numpy array\n",
    "# ADCs x channels x time samples\n",
    "all_samples = np.zeros(\n",
    "    (total_ADCs, total_channels, (end_seq_no - start_seq_no + 64)),\n",
    "    dtype=np.complex64,\n",
    ")\n",
    "all_samples_scaled = np.zeros(\n",
    "    (total_ADCs, total_channels, (end_seq_no - start_seq_no + 64)),\n",
    "    dtype=np.complex64,\n",
    ")\n",
    "# scale factor for each sample, initialise with -1\n",
    "all_scales = -500 * np.ones(\n",
    "    (total_ADCs, total_channels, (end_seq_no - start_seq_no + 64)), dtype=np.float32\n",
    ")\n",
    "pkt_scale = np.zeros(total_ADCs, dtype=np.float32)\n",
    "pcount = 0\n",
    "IS_FIRST_PACKET = True\n",
    "for ts, pkt_payload in zip(tstamps, payloads):\n",
    "    pcount += 1\n",
    "    seq_no = struct.unpack(\"<Q\", pkt_payload[0:8])[0]\n",
    "    FPGA_id = struct.unpack(\"<I\", pkt_payload[8:12])[0]\n",
    "    freq_chan = struct.unpack(\"<H\", pkt_payload[12:14])[0] - start_chan\n",
    "    padding = struct.unpack(\"<Q\", pkt_payload[14:22])[0]\n",
    "\n",
    "    # Get scale factors\n",
    "    for adc in range(total_ADCs):\n",
    "        pkt_scale[adc] = np.float32(\n",
    "            struct.unpack(\"H\", pkt_payload[(22 + 2 * adc) : (24 + 2 * adc)])[0]\n",
    "        )\n",
    "    # Get data\n",
    "    data_base = 22 + total_ADCs * 2\n",
    "    for adc in range(total_ADCs):\n",
    "        for t in range(64):  # 64 time samples per packet\n",
    "            x_i, x_q = struct.unpack(\n",
    "                \">bb\",\n",
    "                pkt_payload[\n",
    "                    (data_base + t * total_ADCs * 2 + adc * 2) : (\n",
    "                        data_base + t * total_ADCs * 2 + adc * 2 + 2\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "            all_samples[adc, freq_chan, seq_no - start_seq_no + t] = (\n",
    "             #   1j * np.float32(x_q) + np.float32(x_i)\n",
    "                1j * x_q + x_i\n",
    "            )\n",
    "            # all_samples_scaled[adc, freq_chan, seq_no - start_seq_no + t] = (\n",
    "            #     pkt_scale[adc] * (1j * np.float32(x_q) + np.float32(x_i))\n",
    "            # )\n",
    "            #all_scales[adc, freq_chan, seq_no - start_seq_no + t] = pkt_scale[adc]\n",
    "    if pcount >= pcount_max:\n",
    "        print(f\"stopping packet decoding at packet {pcount}\")\n",
    "        break\n",
    "\n",
    "all_samples = all_samples.transpose((1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "59f1e494-532e-464e-8b9d-a49fd4f696d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['packet_samples']\n",
      "Shape: (8, 16, 64, 20)\n",
      "Data type: [('r', 'i1'), ('i', 'i1')]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Open file (read-only mode)\n",
    "with h5py.File(\"first_buffer.hdf5\", \"r\") as f:\n",
    "    # List all groups\n",
    "    print(\"Keys:\", list(f.keys()))\n",
    "    \n",
    "    # Access a dataset\n",
    "    dataset = f[\"packet_samples\"]\n",
    "    print(\"Shape:\", dataset.shape)\n",
    "    print(\"Data type:\", dataset.dtype)\n",
    "    \n",
    "    # Load data into memory (NumPy array)\n",
    "    data = dataset[:]\n",
    "\n",
    "data = data.transpose((0,1,3,2))\n",
    "data = data['r'].astype(np.float32) + 1j * data['i'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6484a478-d694-43c4-99a3-2aceee195df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13.+18.j, -46. -1.j,  34. +5.j,  91.+16.j, -36. -6.j,  28.-28.j,\n",
       "        19.-40.j, -50.-20.j,  -6.-24.j,   5.+11.j,   0.+48.j,  15.+15.j,\n",
       "       -20. +2.j,   9. +1.j,  14. -7.j, -41.-41.j,  -5.-40.j, -35.-18.j,\n",
       "       -69. +9.j, -14.+38.j, -15.-34.j,   0. -3.j, -34.-34.j,  -8.-30.j,\n",
       "        -7.+11.j, -24. -8.j,  -1.+32.j, -46.-45.j,  21.-44.j,   6.+28.j,\n",
       "         8. -2.j, -29.-20.j, -13.-17.j, -25. -3.j,  -2. -4.j,  12.-29.j,\n",
       "       -24. +4.j,  -4.+12.j, -19. +1.j,   6.-39.j,   6.-25.j,   3.+14.j,\n",
       "       -13.-28.j,  15.-15.j,  25.+21.j,  10.+13.j, -21. +6.j,  13.-16.j,\n",
       "        25.+26.j, -14.-11.j,  39.-32.j,   6. -7.j,  18.+40.j,  17.-20.j,\n",
       "         4.-20.j, -16. -3.j,  -1.-34.j,  48.-45.j,  18.+12.j, -20.+23.j,\n",
       "         5. -1.j,   3.+37.j,  -8.+24.j,   1.-37.j], dtype=complex64)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[4][0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3f503711-7d09-4263-8b2f-b35133fa17a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13.+18.j, -46. -1.j,  34. +5.j,  91.+16.j, -36. -6.j,  28.-28.j,\n",
       "        19.-40.j, -50.-20.j,  -6.-24.j,   5.+11.j,   0.+48.j,  15.+15.j,\n",
       "       -20. +2.j,   9. +1.j,  14. -7.j, -41.-41.j,  -5.-40.j, -35.-18.j,\n",
       "       -69. +9.j, -14.+38.j, -15.-34.j,   0. -3.j, -34.-34.j,  -8.-30.j,\n",
       "        -7.+11.j, -24. -8.j,  -1.+32.j, -46.-45.j,  21.-44.j,   6.+28.j,\n",
       "         8. -2.j, -29.-20.j, -13.-17.j, -25. -3.j,  -2. -4.j,  12.-29.j,\n",
       "       -24. +4.j,  -4.+12.j, -19. +1.j,   6.-39.j,   6.-25.j,   3.+14.j,\n",
       "       -13.-28.j,  15.-15.j,  25.+21.j,  10.+13.j, -21. +6.j,  13.-16.j,\n",
       "        25.+26.j, -14.-11.j,  39.-32.j,   6. -7.j,  18.+40.j,  17.-20.j,\n",
       "         4.-20.j, -16. -3.j,  -1.-34.j,  48.-45.j,  18.+12.j, -20.+23.j,\n",
       "         5. -1.j,   3.+37.j,  -8.+24.j,   1.-37.j], dtype=complex64)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples[4][5][:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0a1e57df-c948-4d38-8396-153ee8dd324c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 20, 825152)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dc1a8e03-05a4-4c4b-960c-7468f4977951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 16, 20, 64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d474032c-7d48-4383-9bf7-e28cfbf3cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    for j in range(20):\n",
    "        for k in range(16):\n",
    "            assert np.isclose(data[i][k][j], all_samples[i][j][k * 64: (k+1) * 64]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f27201f-d37b-4d3e-af52-cfc16f0a48f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
